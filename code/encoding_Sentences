from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import os.path
import scipy.spatial.distance as sd
import configuration
import encoder_manager
import tensorflow as tf
import json
from nltk.tokenize import word_tokenize

FLAGS = tf.flags.FLAGS

MR_DATA_DIR = "/home/shencz/data/rt-polaritydata/"

tf.flags.DEFINE_string("model_config", "/home/shencz/code/S2V-master/model_configs/BS400-W620-S1200-case-bidir/eval_noexp.json", "Model configuration json")
tf.flags.DEFINE_string("results_path", "/home/shencz/data/models02/", "Model results path")


#tf.flags.DEFINE_string("data_dir", None, "Directory containing training data.")
tf.flags.DEFINE_float("uniform_init_scale", 0.1, "Random init scale")
tf.flags.DEFINE_integer("batch_size", 400, "Batch size")
tf.flags.DEFINE_boolean("use_norm", False,
                        "Normalize sentence embeddings during evaluation")
tf.flags.DEFINE_integer("sequence_length", 30, "Max sentence length considered")
tf.flags.DEFINE_string("Glove_path", None, "Path to Glove dictionary")

encoder = encoder_manager.EncoderManager()

with open(FLAGS.model_config) as json_config_file:
    model_config = json.load(json_config_file)
encoder.load_model(configuration.model_config(model_config, mode="encode"))

data = []
with open("/home/shencz/data/test04.txt", 'rb') as f:
  data.extend([line.decode('latin-1').strip() for line in f])

#print(data)
encodings = encoder.encode(data)


def get_nn(ind, num=999):
  encoding = encodings[ind]
  scores = sd.cdist([encoding], encodings, "cosine")[0]
  #print(scores)
  sorted_ids = np.argsort(scores)
  print("Sentence:")
  print("", data[ind])
  #print(encoding)
  print("\nNearest neighbors:")
  for i in range(1, num + 1):
    print(" %d. %s (%.3f)" %
          (i, data[sorted_ids[i]], scores[sorted_ids[i]]))

get_nn(0)
